{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss:19.3194633961 eval loss:18.8017462081\n",
      "train loss:2.84147881878 eval loss:2.97459431931\n",
      "train loss:1.0359546407 eval loss:1.69574160581\n",
      "train loss:0.801298192954 eval loss:1.51507842818\n",
      "train loss:0.595212806622 eval loss:1.23684854896\n",
      "train loss:0.455058799601 eval loss:1.01216269812\n",
      "train loss:0.316083545359 eval loss:0.625711090344\n",
      "train loss:0.278980330454 eval loss:0.565753131321\n",
      "train loss:0.252378626266 eval loss:0.533097980573\n",
      "train loss:0.230877445857 eval loss:0.508241041719\n",
      "train loss:0.213067308434 eval loss:0.490379534013\n",
      "train loss:0.19900389337 eval loss:0.48545387541\n",
      "train loss:0.197195548902 eval loss:0.531555652588\n",
      "train loss:0.323680026727 eval loss:0.924071176516\n",
      "train loss:0.207584753745 eval loss:0.594317122748\n",
      "train loss:0.158180405118 eval loss:0.416597577206\n",
      "train loss:0.148810748977 eval loss:0.404831641687\n",
      "train loss:0.141290754624 eval loss:0.396454436158\n",
      "train loss:0.134832264171 eval loss:0.389170649099\n",
      "train loss:0.129157665089 eval loss:0.382633553185\n",
      "train loss:0.124124833147 eval loss:0.376648162364\n",
      "train loss:0.119643737875 eval loss:0.371060119295\n",
      "train loss:0.11564505768 eval loss:0.36574333904\n",
      "train loss:0.112074341494 eval loss:0.360593749218\n",
      "train loss:0.108885427181 eval loss:0.355533064715\n",
      "train loss:0.106036396816 eval loss:0.35053864404\n",
      "train loss:0.103489608938 eval loss:0.345616257783\n",
      "train loss:0.101214925611 eval loss:0.340815105028\n",
      "train loss:0.0991831746743 eval loss:0.336264995351\n",
      "train loss:0.097384470928 eval loss:0.332219565459\n",
      "train loss:0.0958636026522 eval loss:0.329211198639\n",
      "train loss:0.0948501498608 eval loss:0.328417528842\n",
      "train loss:0.0951301417613 eval loss:0.332525845474\n",
      "train loss:0.0985896685288 eval loss:0.346812701692\n",
      "train loss:0.106864164849 eval loss:0.374706831257\n",
      "train loss:0.111327538246 eval loss:0.388329601513\n",
      "train loss:0.0954245076945 eval loss:0.332964490677\n",
      "train loss:0.0887692733432 eval loss:0.303306081274\n",
      "train loss:0.0871264327804 eval loss:0.293629619909\n",
      "train loss:0.0862288589371 eval loss:0.287920237667\n",
      "train loss:0.0855279791476 eval loss:0.283330799555\n",
      "train loss:0.0849227774605 eval loss:0.279199875617\n",
      "train loss:0.0843842178785 eval loss:0.275322980276\n",
      "train loss:0.0839000504837 eval loss:0.271625359451\n",
      "train loss:0.083463268411 eval loss:0.26807722613\n",
      "train loss:0.0830685545435 eval loss:0.264664688706\n",
      "train loss:0.0827121748885 eval loss:0.26138029742\n",
      "train loss:0.0823908977372 eval loss:0.258218184212\n",
      "train loss:0.0821020651832 eval loss:0.255171876913\n",
      "train loss:0.0818433303173 eval loss:0.252236125354\n",
      "train loss:0.0816125896792 eval loss:0.249404658739\n",
      "train loss:0.0814080873482 eval loss:0.246671682378\n",
      "train loss:0.0812280855634 eval loss:0.244032596922\n",
      "train loss:0.0810709371031 eval loss:0.241483781415\n",
      "train loss:0.0809341160484 eval loss:0.239022448093\n",
      "train loss:0.0808166866848 eval loss:0.236643064271\n",
      "train loss:0.0807178000417 eval loss:0.234343112067\n",
      "train loss:0.0806368706089 eval loss:0.232120786601\n",
      "train loss:0.0805728675496 eval loss:0.229972253717\n",
      "train loss:0.0805258324277 eval loss:0.227887302988\n",
      "train loss:0.0804944950064 eval loss:0.225873494019\n",
      "train loss:0.0804771405567 eval loss:0.223928549161\n",
      "train loss:0.0804731581167 eval loss:0.222049797886\n",
      "train loss:0.080482009024 eval loss:0.220234316628\n",
      "train loss:0.0805041844711 eval loss:0.21847886557\n",
      "train loss:0.0805383689532 eval loss:0.216782586215\n",
      "train loss:0.0805836059625 eval loss:0.215143743063\n",
      "train loss:0.0806391823164 eval loss:0.213559925421\n",
      "train loss:0.0807045210214 eval loss:0.212028679043\n",
      "train loss:0.0807789277432 eval loss:0.210548111638\n",
      "train loss:0.0808618786063 eval loss:0.209117000366\n",
      "train loss:0.0809522290865 eval loss:0.207733643174\n",
      "train loss:0.0810506593543 eval loss:0.206390957996\n",
      "train loss:0.0811566130754 eval loss:0.205086421886\n",
      "train loss:0.081268420984 eval loss:0.203823837748\n",
      "train loss:0.0813873735063 eval loss:0.202602465656\n",
      "train loss:0.0815128086871 eval loss:0.201420640022\n",
      "train loss:0.0816440708008 eval loss:0.200276781242\n",
      "train loss:0.0817815074089 eval loss:0.199169343995\n",
      "train loss:0.0819244214361 eval loss:0.198096982302\n",
      "train loss:0.0820726965227 eval loss:0.197057724787\n",
      "train loss:0.0822259496894 eval loss:0.196050678725\n",
      "train loss:0.0823840462141 eval loss:0.19507558271\n",
      "train loss:0.082544680608 eval loss:0.194129108168\n",
      "train loss:0.0827095446692 eval loss:0.193213713816\n",
      "train loss:0.0828780106513 eval loss:0.192325893464\n",
      "train loss:0.0830497679295 eval loss:0.19146307447\n",
      "train loss:0.0832255678257 eval loss:0.19063036805\n",
      "train loss:0.0834053422682 eval loss:0.189832146317\n",
      "train loss:0.0835881250319 eval loss:0.189056921268\n",
      "train loss:0.083774832348 eval loss:0.188304435196\n",
      "train loss:0.0839644214445 eval loss:0.187575956008\n",
      "train loss:0.0841573126508 eval loss:0.186870476761\n",
      "train loss:0.0843546361989 eval loss:0.186187305652\n",
      "train loss:0.0845550965377 eval loss:0.185526361845\n",
      "train loss:0.0847576751413 eval loss:0.184886251596\n",
      "train loss:0.0849635127153 eval loss:0.184266350957\n",
      "train loss:0.0851730979222 eval loss:0.183664779335\n",
      "train loss:0.0853851578554 eval loss:0.183081187196\n",
      "train loss:0.0855994098637 eval loss:0.182515196773\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "from utils import *\n",
    "convweight = []\n",
    "krow = 3\n",
    "kcol = 2\n",
    "kernelsize = 5\n",
    "for i in range(0, kernelsize):\n",
    "    convweight.append(np.random.random(krow * kcol) - 0.5)\n",
    "fcweight = np.random.random((42, 10)) - 0.5\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits['images'][:-100]\n",
    "Y = digits['target'][:-100]\n",
    "X_batch_sum = conv_2d_to_matrix_batch(X, krow, kcol)\n",
    "X_te = digits['images'][-100:]\n",
    "Y_te = digits['target'][-100:]\n",
    "Y_dummy = np.zeros((len(X), 10))\n",
    "for i in range(0, len(X)):\n",
    "    Y_dummy[i, Y[i]] = 1\n",
    "    \n",
    "    \n",
    "def conv_1d_to_2d(data, row, column):\n",
    "    if len(data) != row * column:\n",
    "        return\n",
    "    res = np.zeros((row, column))\n",
    "    for i in range(0, row):\n",
    "        for j in range(0, column):\n",
    "            res[i][j] = data[i*column + j]\n",
    "    return res\n",
    "\n",
    "\n",
    "def conv_2d_to_matrix(data, kernelrow, kernelcol):\n",
    "    mid = []\n",
    "    for xl in range(0, data.shape[0] - kernelrow + 1):\n",
    "        for yl in range(0, data.shape[1] - kernelcol + 1):\n",
    "            mid.append(data[xl:xl+kernelrow, yl:yl+kernelcol].flatten())\n",
    "    return np.array(mid)\n",
    "\n",
    "\n",
    "\n",
    "def conv_2d_to_matrix_batch(data, kernelrow, kernelcol):\n",
    "    mid = []\n",
    "    for single in data:\n",
    "        mid.append(conv_2d_to_matrix(single, kernelrow, kernelcol))\n",
    "    return np.array(mid)\n",
    "\n",
    "def recover_matrix_to_2d(data, row, col):\n",
    "    if (len(data) != row * col):\n",
    "        return\n",
    "    res = np.zeros((row, col))\n",
    "    idx = 0\n",
    "    for i in range(0, row):\n",
    "        for j in range(0, col):\n",
    "            res[i, j] = data[idx]\n",
    "            idx += 1\n",
    "    return res\n",
    "\n",
    "def recover_matrix_to_2d_batch(data, row, col):\n",
    "    res = []\n",
    "    for d in data:\n",
    "        res.append(recover_matrix_to_2d(d, row, col))\n",
    "    return np.array(res)\n",
    "\n",
    "\n",
    "#2d matrix with kernellist and recover\n",
    "def conv(data, kernellist):\n",
    "    res = None\n",
    "    resrow = data.shape[0] - krow + 1\n",
    "    rescol = data.shape[1] - kcol + 1\n",
    "    kernelmtx = np.sum(kernellist, axis=0)\n",
    "    res = np.dot(conv_2d_to_matrix(data, krow, kcol), kernelmtx)\n",
    "#     ###for kernel in kernellist:\n",
    "#         tmp = np.dot(conv_2d_to_matrix(data, krow, kcol), kernel)\n",
    "#         if res is None:\n",
    "#             res = tmp\n",
    "#         else:\n",
    "#             res += tmp\n",
    "    return recover_matrix_to_2d(res, resrow, rescol)\n",
    "\n",
    "def conv_batch(data, kernellist):\n",
    "    res = []\n",
    "    x_batch_sum = conv_2d_to_matrix_batch(data, krow, kcol)\n",
    "    #print x_batch_sum.shape\n",
    "    resrow = data.shape[0] - krow + 1\n",
    "    rescol = data.shape[1] - kcol + 1\n",
    "    kernelmtx = np.sum(kernellist, axis=0)\n",
    "    return np.tensordot(x_batch_sum, kernelmtx, [2, 0])#.shape\n",
    "\n",
    "def conv_batch_sum(data, data_cov, kernellist):\n",
    "    res = []\n",
    "    #print x_batch_sum.shape\n",
    "    resrow = data.shape[0] - krow + 1\n",
    "    rescol = data.shape[1] - kcol + 1\n",
    "    kernelmtx = np.sum(kernellist, axis=0)\n",
    "    return np.tensordot(data_cov, kernelmtx, [2, 0])#.shape\n",
    "#     for d in data:\n",
    "#         res.append(conv(d, kernellist))\n",
    "#     print np.array(res).shape\n",
    "#     return np.array(res)\n",
    "    \n",
    "\n",
    "def forward(x):\n",
    "    #conv, relu\n",
    "    tmpmtx = relu(conv_batch(x, convweight).reshape(len(x), 42))\n",
    "    #fc\n",
    "    return softmax(np.dot(tmpmtx, fcweight))\n",
    "    \n",
    "regu = 0.01\n",
    "    \n",
    "lr = 0.05\n",
    "for iter in range(0,1000):\n",
    "    batchsize = len(X)\n",
    "    for i in range(0, X.shape[0], batchsize):\n",
    "        x_batch = X[i:i+batchsize, :]\n",
    "        y_batch = Y[i:i+batchsize]\n",
    "        y_dummy_batch = Y_dummy[i:i+batchsize, :]\n",
    "        #print 'pre'\n",
    "        x_batch_sum = X_batch_sum[i:i+batchsize]\n",
    "        #print 'batch'\n",
    "        #print 'xbatch'\n",
    "        #print x_batch_sum.shape\n",
    "        #forward\n",
    "        #conv + relu\n",
    "        x_conv = conv_batch_sum(x_batch, x_batch_sum, convweight)#.reshape(len(x_batch), 42)\n",
    "        #print 'conv'\n",
    "        z = relu(x_conv)\n",
    "        #fc\n",
    "        a2 = z.dot(fcweight)\n",
    "        #out\n",
    "        pre_batch = softmax(a2)\n",
    "        batch_loss = softmax_loss(pre_batch, y_batch)\n",
    "        if iter % 10 == 0:\n",
    "            print 'train loss:' + str(batch_loss) + ' eval loss:' + str(softmax_loss(forward(X_te), Y_te))\n",
    "        grad_a2 = (pre_batch - y_dummy_batch) / batchsize\n",
    "        grad_w2 = (z.T.dot(grad_a2) + regu * fcweight)\n",
    "        grad_z = grad_a2.dot(fcweight.T)\n",
    "        grad_z_a = z.copy()\n",
    "        grad_z_a[grad_z_a > 0] = 1\n",
    "        grad_z_a[grad_z_a != 1] = 0\n",
    "        #print grad_z.shape\n",
    "        #grad_conv = np.mean((grad_z * grad_z_a).dot(x_batch_sum), axis=0)\n",
    "        grad_conv = np.tensordot(x_batch_sum, (grad_z * grad_z_a), ([0, 1], [0, 1])) / len(x_batch_sum)\n",
    "        fcweight -= lr * grad_w2\n",
    "        for i in range(0, kernelsize):\n",
    "            convweight[i] -= lr * (grad_conv + regu * convweight[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
