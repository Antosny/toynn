{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1697, 10)\n",
      "loss:30.5699718003\n",
      "(1697, 10)\n",
      "loss:31.038087214\n",
      "(1697, 10)\n",
      "loss:27.2321053721\n",
      "(1697, 10)\n",
      "loss:26.4790501413\n",
      "(1697, 10)\n",
      "loss:27.8426906943\n",
      "(1697, 10)\n",
      "loss:31.038087214\n",
      "(1697, 10)\n",
      "loss:29.3488011558\n",
      "(1697, 10)\n",
      "loss:27.3338695924\n",
      "(1697, 10)\n",
      "loss:24.9729396798\n",
      "(1697, 10)\n",
      "loss:27.9037492265\n",
      "(1697, 10)\n",
      "loss:25.9295233513\n",
      "(1697, 10)\n",
      "loss:28.7789215217\n",
      "(1697, 10)\n",
      "loss:26.947165555\n",
      "(1697, 10)\n",
      "loss:27.4559866569\n",
      "(1697, 10)\n",
      "loss:28.1683361995\n",
      "(1697, 10)\n",
      "loss:25.9702290394\n",
      "(1697, 10)\n",
      "loss:27.5170451891\n",
      "(1697, 10)\n",
      "loss:24.1588259168\n",
      "(1697, 10)\n",
      "loss:23.1818894012\n",
      "(1697, 10)\n",
      "loss:25.6649363783\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "from sklearn.datasets import load_digits\n",
    "import numpy as np\n",
    "hidden = 20\n",
    "\n",
    "digits = load_digits()\n",
    "\n",
    "X = digits['images'][:-100]\n",
    "Y = digits['target'][:-100]\n",
    "X_te = digits['images'][-100:]\n",
    "Y_te = digits['target'][-100:]\n",
    "Y_dummy = np.zeros((len(X), 10))\n",
    "for i in range(0, len(X)):\n",
    "    Y_dummy[i, Y[i]] = 1\n",
    "\n",
    "w1 = np.random.random((X[0].shape[0], hidden)) - 0.5\n",
    "w2 = np.random.random((hidden, hidden)) - 0.5\n",
    "w3 = np.random.random((hidden, 10)) - 0.5\n",
    "\n",
    "#final output label\n",
    "def forward(x):\n",
    "    preh = None\n",
    "    a = x.T.dot(w1)\n",
    "    print a.shape\n",
    "    for data in a:\n",
    "        if preh is None:\n",
    "            h = relu(data)\n",
    "        else:\n",
    "            h = relu(data + preh.dot(hidden))\n",
    "        preh = h\n",
    "    return softmax([preh.dot(w3)])\n",
    "\n",
    "lr = 0.00001\n",
    "\n",
    "predhlist = []\n",
    "\n",
    "for j in range(0, len(X)):\n",
    "    x = X[j]\n",
    "    y = Y[j]\n",
    "    y_du = Y_dummy[j]\n",
    "    a = x.T.dot(w1)\n",
    "    preh = None\n",
    "    hlist = []\n",
    "    for data in a:\n",
    "        if preh is None:\n",
    "            h = relu(data)\n",
    "        else:\n",
    "            h = relu(data + preh.dot(hidden))\n",
    "        preh = h\n",
    "        hlist.append(h.reshape(1, hidden))\n",
    "    predhlist.append(hlist[-1].reshape(hidden))\n",
    "\n",
    "predarray = np.array(predhlist)\n",
    "\n",
    "for iter in range(0, 20):\n",
    "    pre_batch = softmax(predarray.dot(w3))\n",
    "    print pre_batch.shape\n",
    "    batch_loss = softmax_loss(pre_batch, Y)\n",
    "    print 'loss:' + str(batch_loss)\n",
    "    grada2 = (pre_batch - Y_dummy) / len(X)\n",
    "    gradw2 = predarray.T.dot(grada2)\n",
    "    w3 -= lr * gradw2\n",
    "#     pred = []\n",
    "#     w3grad = []\n",
    "#     for i in range(0, len(X)):\n",
    "#         p = softmax([predhlist[i].dot(w3).reshape(10)])\n",
    "#         pred.append(p.reshape(10))\n",
    "#         gradsoft = p - Y_dummy[i]\n",
    "#         w3grad.append(predhlist[i].T.dot(gradsoft))\n",
    "#         #w3 -= lr * w3grad[-1]\n",
    "#     print softmax_loss(np.array(pred), Y)\n",
    "#     w3 -= lr * np.mean(w3grad, axis=0)\n",
    "\n",
    "# for iter in range(0,20):\n",
    "#     batchsize = len(X)\n",
    "#     print 'epoch:' + str(iter)\n",
    "#     print 'lenpredh:' + str(len(predhlist))\n",
    "#     for i in range(0, X.shape[0], batchsize):\n",
    "#         x_batch = X[i:i+batchsize, :]\n",
    "#         y_batch = Y[i:i+batchsize]\n",
    "#         y_dummy_batch = Y_dummy[i:i+batchsize, :]\n",
    "#         #batch\n",
    "#         w1grad = []\n",
    "#         w2grad = []\n",
    "#         w3grad = []\n",
    "#         pred = []\n",
    "#         for j in range(0, len(x_batch)):\n",
    "#             x = x_batch[j]\n",
    "#             y = y_batch[j]\n",
    "#             y_du = y_dummy_batch[j]\n",
    "#             #forward\n",
    "#             a = x.T.dot(w1)\n",
    "#             hlist = []\n",
    "#             gradhlist = []\n",
    "#             gradh = [] #need to reverse in later\n",
    "#             preh = None\n",
    "#             for data in a:\n",
    "#                 if preh is None:\n",
    "#                     h = relu(data)\n",
    "#                 else:\n",
    "#                     h = relu(data + preh.dot(hidden))\n",
    "#                 preh = h\n",
    "#                 gradrelu = preh.copy()\n",
    "#                 gradrelu[gradrelu > 0] = 1\n",
    "#                 gradrelu[gradrelu != 1] = 0\n",
    "#                 gradhlist.append(gradrelu)\n",
    "#                 hlist.append(h.reshape(1, 5))\n",
    "#             if iter == 0:\n",
    "#                 predhlist.append(hlist[-1])\n",
    "#             p = softmax([hlist[-1].dot(w3).reshape(10)])\n",
    "#             if iter > 0:\n",
    "#                 p = softmax(predhlist[j].dot(w3))\n",
    "#             pred.append(p.reshape(10))\n",
    "#             gradsoft = p - y_du\n",
    "#             for l in range(len(hlist) - 1, -1, -1):\n",
    "#                 #output\n",
    "#                 corridx = len(hlist) - l - 1\n",
    "#                 if l == len(hlist) - 1:\n",
    "#                     gradh.append(gradsoft.dot(w3.T).reshape(1, 5))\n",
    "#                 else:\n",
    "#                     gradrelu = gradhlist[corridx]\n",
    "#                     gradh.append((gradrelu * gradh[-1]).dot(w2.T))\n",
    "#             gradh.reverse()\n",
    "#             gradw3 = predhlist[j].T.dot(gradsoft)\n",
    "#             w3grad.append(gradw3)\n",
    "#             #cal grad w2, gradw2 = gradh2 * h2\n",
    "#             tempw2 = []\n",
    "#             for i in range(0, len(hlist)):\n",
    "#                 gradrelu = gradhlist[corridx]\n",
    "#                 tempw2.append(hlist[i].T.dot(gradh[i] * gradrelu))\n",
    "#             w2grad.append(np.array(tempw2).mean(axis=0))\n",
    "#             #cal grad w1\n",
    "#             tempw1 = []\n",
    "#             for i in range(0, len(hlist)):\n",
    "#                 gradrelu = gradhlist[corridx]\n",
    "#                 tempw1.append(x[i].reshape(8, 1).dot(gradh[i] * gradrelu))\n",
    "#             w1grad.append(np.array(tempw1).mean(axis=0))\n",
    "#         print softmax_loss(np.array(pred), y_batch)\n",
    "#         #w1 = w1 - lr * np.mean(w1grad, axis=0)\n",
    "#         #w2 = w2 - lr * np.mean(w2grad, axis=0)\n",
    "#         w3 -= lr * np.mean(w3grad, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
